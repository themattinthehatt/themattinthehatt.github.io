<!DOCTYPE html>
<html class="gray-page">
<!-- purple-page -->

<!--embed google fonts-->
<link href="https://fonts.googleapis.com/css?family=Cutive+Mono|Khula" rel="stylesheet">

<head>
	<meta http-equiv="content-type" content="text/html" charset="utf-8" />
	<link type="text/css" rel="stylesheet" href="css/stylesheet.css" />
	<title>matt whiteway | research</title>
    <link rel="icon" href="images/favicon.png">
</head>

<body class="centered">
	<nav class="centered" style="max-width: 1000px">
		<ul class="centered">
			<li><a class="link-purp" href="index.html">about</a></li>
			<li><a class="link-purp" href="research.html">research</a></li>
            <li><a class="link-purp" href="https://github.com/themattinthehatt">code</a></li>
			<!-- <li><a href="/teaching.html">teaching</a></li> -->
			<li><a class="link-purp" href="docs/cv.pdf">cv</a></li>
			<li><a class="link-purp" href="art.html">fun</a></li>
			<!--<li><a class="link-purp" href="links.html">links</a></li>-->
		</ul>
	</nav>
	<!-- ---------------------------- INTRO ------------------------ -->
	<div class="container centered" style="max-width: 1000px">
		<div class="center-sub-container" style="width: 100%">
			<div class="center-blurb">
				<h2>Research Interests</h2>
                <p>I am interested in animal behavior and how it is controlled by the brain.
                    To study this, I develop tools for quantifying animal behavior from video data,
                    for example pose estimation and action segmentation.
                    I also develop tools for connecting that behavior to simultaneously recorded
                    neural activity in order to better understand the neural information processing
                    that underlies complex adaptive behavior.
                </p>
<!--				<p>As we move through the world around us our senses collect huge amounts-->
<!--                    of information about the state of our environment. The brain-->
<!--                    must process this information quickly and accurately in order to make-->
<!--                    decisions, implement coordinated motor responses, and predict future-->
<!--                    events. How does the brain do this? I am interested in studying sensory-->
<!--                    processing systems in order to better understand the fundamental-->
<!--                    computations that neurons perform during the processing of external-->
<!--                    stimuli. I am also interested in how internal states such as motivation-->
<!--                    and attention influence these computations.</p>-->
<!--				<p>More broadly, I am interested in how the fundamental principles-->
<!--                    underlying these computational strategies employed by the brain can be-->
<!--                    translated into artificial information processing systems. Over the last-->
<!--                    decade it has become clear that powerful new machine learning approaches-->
<!--                    like deep neural networks have a superficial but strong connection to-->
<!--                    biological information processing systems, like the visual system.-->
<!--                    Further developing these parallels will inevitably result in significant-->
<!--                    scientific and technological advances in the years to come.-->
<!--				</p>-->
                <p>
                    <a class="doc-link" href="https://scholar.google.com/citations?user=jeIGzDIAAAAJ&hl=en">Google Scholar</a>
                </p>
			</div>
		</div>
	</div>
	<!--<div class="container centered" style="max-width: 1000px;-->
		<!--background-color: transparent; border: transparent">-->
		<!--<div class="center-sub-container" style="width: 100%">-->
			<!--<div class="center-blurb" style="text-align: center">-->
				<!--<h2>PROJECTS</h2>-->
			<!--</div>-->
		<!--</div>-->
	<!--</div>-->
	<div style="width: 100%; margin-bottom: 20px">
	</div>

    <!-- --------------------------- LIGHTNING POSE -------------- -->
    <div class="container centered", style="max-width: 1000px">
        <div class="left-sub-container" style="width: 60%">
            <div class="left-blurb" style="padding-top: 0px; padding-right: 60px">
                <h2>Lightning Pose</h2>
                <p>Lightning Pose (LP) is an industry-grade pose estimation software package that
                    utilizes unlabeled frames to train better models. LP uses unsupervised training
                    objectives that penalize the network whenever its predictions violate a variety
                    of spatiotemporal constraints. It also uses a new network architecture that
                    predicts the pose for a given frame using temporal context from surrounding
                    unlabeled frames. We provide a browser-based app for data labeling and
                    model training and evaluation.
                </p>
                <a class="doc-link" href="https://www.biorxiv.org/content/10.1101/2023.04.28.538703v1">preprint</a> |
                <a class="doc-link" href="https://github.com/danbider/lightning-pose">code</a> |
                <a class="doc-link" href="https://github.com/Lightning-Universe/Pose-app">app</a>
            </div>

        </div>
        <div class="right-sub-container" style="width: 36%">
            <div class="center-image">
                <img src="images/LightningPose_horizontal_light.png" style="border-radius: 4px"/>
            </div>
        </div>
    </div>
    <div style="width: 100%; margin-bottom: 20px">
    </div>

    <!-- --------------------------- daart -------------- -->
    <div class="container centered", style="max-width: 1000px">
        <div class="left-sub-container" style="width: 60%">
            <div class="left-blurb" style="padding-top: 0px; padding-right: 60px">
                <h2>Semi-supervised behavioral segmentation</h2>
                <p>We introduce a semi-supervised approach to behavioral segmentation that
                    leverages weak- and self-supervision. With this approach we show that a large
                    number of unlabeled frames can improve supervised segmentation in the regime of
                    sparse hand labels, and also show that a small number of hand labeled frames
                    can increase the precision of unsupervised segmentation.</p>
                <a class="doc-link" href="https://www.biorxiv.org/node/2013046.full">preprint</a> |
                <a class="doc-link" href="https://github.com/themattinthehatt/daart">code</a>
            </div>

        </div>
        <div class="right-sub-container" style="width: 36%">
            <div class="center-image">
                <img src="images/daart2.png" style="border-radius: 4px"/>
            </div>
        </div>
    </div>
    <div style="width: 100%; margin-bottom: 20px">
    </div>

    <!-- --------------------------- PS-VAE -------------- -->
    <div class="container centered", style="max-width: 1000px">
        <div class="left-sub-container" style="width: 60%">
            <div class="left-blurb" style="padding-top: 0px; padding-right: 60px">
                <h2>Partitioned Subspace VAE</h2>
                <p>The PS-VAE is a behavioral video analysis tool that combines the output of
                    supervised pose estimation algorithms (e.g. DeepLabCut) with unsupervised
                    dimensionality reduction methods (e.g. VAEs) to produce interpretable,
                    low-dimensional representations of behavioral videos that extract more
                    information than pose estimates alone. We show how these interpretable
                    representations can be used to characterize the dynamics of behavior, as well
                    as improve the precision of neural decoding analyses.</p>
<!--                <a class="doc-link" href="https://www.biorxiv.org/content/10.1101/2021.02.22.432309v1">preprint</a> |-->
                <a class="doc-link" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009439">paper</a> |
                <a class="doc-link" href="https://github.com/themattinthehatt/behavenet">code</a>
            </div>

        </div>
        <div class="right-sub-container" style="width: 36%">
            <div class="center-image">
                <img src="images/ps-vae_ibl.gif" style="border-radius: 4px"/>
            </div>
        </div>
    </div>
    <div style="width: 100%; margin-bottom: 20px">
    </div>

    <!-- --------------------------- BEHAVENET -------------- -->
    <div class="container centered", style="max-width: 1000px">
        <div class="left-sub-container" style="width: 60%">
            <div class="left-blurb" style="padding-top: 0px; padding-right: 60px">
                <h2>BehaveNet</h2>
                <p>BehaveNet is a probabilistic framework that provides 
                    tools for compression, segmentation, generation, and decoding
                    of behavioral videos. Compression is performed using convolutional 
                    autoencoders (CAEs). An autoregressive hidden Markov model 
                    segments the continuous CAE representation into discrete 
                    â€œbehavioral syllables". Based on this generative model, we 
                    develop a novel Bayesian decoder that takes in neural activity 
                    and outputs probabilistic estimates of the behavioral video.</p>
                <a class="doc-link" href="https://papers.nips.cc/paper/9701-behavenet-nonlinear-embedding-and-bayesian-neural-decoding-of-behavioral-videos">paper</a> |
                <a class="doc-link" href="https://github.com/themattinthehatt/behavenet">code</a>
            </div>

        </div>
        <div class="right-sub-container" style="width: 36%">
            <div class="right-image">
                <img src="images/behavenet.png" style="border-radius: 4px"/>
            </div>
        </div>
    </div>
    <div style="width: 100%; margin-bottom: 20px">
    </div>

    <!-- --------------------------- DECODING PROJECT -------------- -->
    <div class="container centered", style="max-width: 1000px">
        <div class="left-sub-container" style="width: 60%">
            <div class="left-blurb" style="padding-top: 0px; padding-right: 60px">
                <h2>Decoding in the presence of shared neural variability</h2>
                <p>We develop a new decoding framework for estimating stimulus identity
                    from recorded neural population activity. Our framework exploits the
                    low-dimensional structure of this activity, resulting in a linear
                    estimator that is more efficient than those from other common linear
                    decoding algorithms. Furthermore, this framework admits a straighforward
                    nonlinear extension that compares favorably to other nonlinear
                    decoding algorithms.</p>
                <a class="doc-link" href="https://www.biorxiv.org/content/10.1101/2020.01.06.896423v1">preprint</a>
                <!--<a class="doc-link" href="https://github.com/themattinthehatt/gam">code</a>-->
            </div>

        </div>
        <div class="right-sub-container" style="width: 36%">
            <div class="right-image">
                <img src="images/trajectories.png" style="border-radius: 4px"/>
            </div>
        </div>
    </div>
    <div style="width: 100%; margin-bottom: 20px">
    </div>

	<!-- --------------------------- GAM PROJECT -------------- -->
	<div class="container centered", style="max-width: 1000px">
		<div class="left-sub-container" style="width: 60%">
			<div class="left-blurb" style="padding-top: 0px; padding-right: 60px">
				<h2>Characterizing shared variability in cortical neuron populations</h2>
				<p>Variability in neural population responses from early sensory areas
                    often contains low-dimensional structure. Here we introduce two new classes of
                    nonlinear latent variable models to characterize this structure. Both
                    model classes rely on autoencoder neural networks for latent variable
                    inference; one class models arbitrary nonlinear interactions while the
                    other explicitly models additive and multiplicative modulations of
                    stimulus responses.</p>
                <a class="doc-link" href="https://nbdt.scholasticahq.com/article/7984-characterizing-the-nonlinear-structure-of-shared-variability-in-cortical-neuron-populations-using-latent-variable-models">paper</a> |
                <a class="doc-link" href="https://github.com/themattinthehatt/whiteway-et-al-2019-nbdt">paper code</a> |
                <a class="doc-link" href="https://github.com/themattinthehatt/gam">model code</a>
			</div>
		</div>
		<div class="right-sub-container" style="width: 36%">
            <div class="right-image">
                <img src="images/gam.png" style="border-radius: 4px"/>
            </div>
		</div>
	</div>
	<div style="width: 100%; margin-bottom: 20px">
	</div>

	<!-- --------------------------- RLVM PROJECT -------------- -->
	<div class="container centered" style ="max-width: 1000px">
		<div class="left-sub-container" style="width: 60%">
            <div class="left-blurb" style="padding-top: 0px; padding-right: 40px">
                <h2>A rectified latent variable model for analyzing neural population recordings</h2>
                <p>We propose the Rectified Latent Variable Model (RLVM) for analyzing
                    neural population activity. The RLVM constrains latent variables to be
                    both rectified and smooth. We demonstrate the advantages of these
                    constraints using both simulated and experimental data, and show how
                    initialization-dependent solutions can be improved by initializing model
                    components with an autoencoder neural network.</p>
                <a class="doc-link" href="http://jn.physiology.org/content/117/3/919">paper</a> |
<!--                <a class="doc-link" href="https://www.biorxiv.org/content/early/2016/08/29/072173">preprint</a> |-->
                <a class="doc-link" href="https://github.com/themattinthehatt/rlvm">code</a>
            </div>
		</div>
		<div class="right-sub-container" style="width: 36%">
            <div class="right-image">
                <img src="images/rlvm_structure.png" style="border-radius: 4px"/>
            </div>
		</div>
	</div>
	<!-- -------------------------- END PROJECTS ----------------- -->

	<footer class="centered">
		<ul class="centered">
			<li>copyright &copy; 2016-2023</li>
			<li>&#x26A1; </li>
			<li>design: matt whiteway</li>
		</ul>
	</footer>
</body>

</html>
